{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From HWGQ-Caffe to HLS: A simple FINN example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING: FINN is undergoing a major overhaul at the moment, so the information below (especially the implementation details) is subject to change.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a pre-trained, binarized network that comes with FINN. As we'll see in a moment, it receives 784 inputs (which is the size of one image in the MNIST dataset), transforms it with three layers of 256 neurons, then produces 10 outputs (one for each digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 1344691 Jun 11 14:22 /app/FINN/../FINN/FINN/inputs/sfc-w1a1.caffemodel\n",
      "-rw-r--r-- 1 root root 2964 Feb 20 14:52 /app/FINN/../FINN/FINN/inputs/sfc-w1a1.prototxt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "FINN_ROOT = os.environ['FINN_ROOT']\n",
    "params = FINN_ROOT + \"/inputs/sfc-w1a1.caffemodel\"\n",
    "topology = FINN_ROOT + \"/inputs/sfc-w1a1.prototxt\"\n",
    "! ls -l $params\n",
    "! ls -l $topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"sfc-w1a1-hwgq\"\r\n",
      "input: \"data\"\r\n",
      "input_shape {\r\n",
      "  dim: 64\r\n",
      "  dim: 1\r\n",
      "  dim: 28\r\n",
      "  dim: 28\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"bn_inp\"\r\n",
      "  type: \"BatchNorm\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"bn_inp\"\r\n",
      "  param {\r\n",
      "    lr_mult: 0\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 0\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 0\r\n",
      "  }\r\n",
      "  batch_norm_param {\r\n",
      "    moving_average_fraction: 0.95\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"qt_inp\"\r\n",
      "  type: \"Quant\"\r\n",
      "  bottom: \"bn_inp\"\r\n",
      "  top: \"qt_inp\"\r\n",
      "  quant_param {\r\n",
      "    forward_func: \"sign\"\r\n",
      "    backward_func: \"hard_tanh\"\r\n",
      "    clip_thr: 1.0\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip1\"\r\n",
      "  type: \"BinaryInnerProduct\"\r\n",
      "  bottom: \"qt_inp\"\r\n",
      "  top: \"ip1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 256\r\n",
      "    bias_term: false\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.005\r\n",
      "    }\r\n",
      "  }\r\n",
      "  binary_inner_product_param {\r\n",
      "    use_alpha: true\r\n",
      "  }  \r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 58 $topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see how the input data is normalized and quantized, and the first binarized fully-connected (here called \"inner product\") layer. Tools such as [Netscope](https://ethereon.github.io/netscope/#/editor) can help understand the structure of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frontend: From Caffe to FINN IR\n",
    "The frontend stage is responsible for converting QNNs trained by a variety of frameworks to the FINN intermediate representation (IR). As each framework exposes their QNN topologies through custom formats, FINN must first perform a conversion to a common IR that it knows how to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import a trained network through the HWGQ-Caffe frontend\n",
    "import FINN.frontend.frontend_hwgq as fe\n",
    "imported_net = fe.importCaffeNetwork(topology, params)\n",
    "imported_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What functionality does the IR expose?\n",
    "Let's get acquainted with what the FINN IR looks like and try to look \"inside\" some of the layers. As FINN is open source you could simply look at the IR source code in `FINN/core/layers.py`, but here we'll use a little helper function to investigate the exposed members and functions dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMembers(layer):\n",
    "    return filter(lambda x: not x.startswith(\"_\"), dir(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W',\n",
       " 'execute',\n",
       " 'getInputSize',\n",
       " 'getNumOps',\n",
       " 'getOutputSize',\n",
       " 'getParamSize',\n",
       " 'getTotalInputBits',\n",
       " 'getTotalOutputBits',\n",
       " 'getTotalParamBits',\n",
       " 'get_filter_dim',\n",
       " 'get_in_dim',\n",
       " 'get_out_dim',\n",
       " 'get_pad',\n",
       " 'get_parallel',\n",
       " 'get_stride',\n",
       " 'get_type',\n",
       " 'ibits',\n",
       " 'in_dim',\n",
       " 'insize',\n",
       " 'kernel',\n",
       " 'obits',\n",
       " 'outsize',\n",
       " 'updateBitwidths',\n",
       " 'wbits']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showMembers(imported_net[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 784, outputs: 256\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs: %d, outputs: %d\" % (imported_net[2].getInputSize(), imported_net[2].getOutputSize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, large parts of the FINN IR are *executable*, as you probably guessed from the `execute` function. At any point, we can generate a random vector of appropriate dimensions and pass it through this layer by calling `execute` to see what the output would be like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rand_inp_vec = np.random.randn(784)\n",
    "ret = imported_net[2].execute(rand_inp_vec)\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get an output vector of the expected size when we pass in an input vector of appropriate size for this layer.\n",
    "\n",
    "## The Streamlining Transform\n",
    "\n",
    "You may have noticed that our imported network contains a bunch of `LinearLayer` instances with floating point parameters (which is currently indicated in FINN IR as 32 bits), like this one:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer type: LinearLayer weight bits: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer type: %s weight bits: %d\" % (imported_net[3].get_type(), imported_net[3].wbits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So where did this come from in a binarized network? Many state-of-the-art BNN/QNN methods use some floating point computation in the forward pass to improve the accuracy. Some examples are batch normalization layers and channel-wise scaling factors. Although these layers do not typically contain a large amount of computation, they may still incur slowdowns on devices where floating point operations are expensive and increase the memory footprint of the QNN by adding floating point parameters. This is the case for creating \"dataflow-style\" FPGA accelerators, so we'd like to somehow get rid of those floating point parameters and operations. \n",
    "\n",
    "Fortunately, we can use what is referred to as [streamlining](https://arxiv.org/pdf/1709.04060.pdf) to do this, without losing any accuracy! Streamlining is implemented as a *transformation* in FINN: a function that takes in the FINN IR representation of a network, and returns a transformed FINN IR representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BipolarThresholdingLayer, FullyConnectedLayer, BipolarThresholdingLayer, FullyConnectedLayer, BipolarThresholdingLayer, FullyConnectedLayer, BipolarThresholdingLayer, FullyConnectedLayer, LinearLayer]\n",
      "Number of layers in original imported network: 17\n",
      "Number of layers in streamlined network: 9\n"
     ]
    }
   ],
   "source": [
    "# example of a device-neutral transform: streamlining\n",
    "import FINN.transforms.transformations as tf\n",
    "streamlined_net = tf.makeCromulent(imported_net)\n",
    "print(streamlined_net)\n",
    "print(\"Number of layers in original imported network: %d\" % len(imported_net))\n",
    "print(\"Number of layers in streamlined network: %d\" % len(streamlined_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all `LinearLayer`s besides the final one have disappeared. This is achieved by updating the thresholds of the network. You can read more about how this is done [here](https://arxiv.org/pdf/1709.04060.pdf), or by looking at the source code for this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input normalization coefficients: [0.01270615] [-0.42501277] \n",
      "Original input quantization threshold: [[0.]]\n",
      "Streamlined input quantization threshold: [[34]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original input normalization coefficients: %s %s \" % (str(imported_net[0].A), str(imported_net[0].B)))\n",
    "print(\"Original input quantization threshold: \" + str(imported_net[1].thresholds))\n",
    "print(\"Streamlined input quantization threshold: \" + str(streamlined_net[0].thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['execute',\n",
       " 'getInputSize',\n",
       " 'getOutputSize',\n",
       " 'get_filter_dim',\n",
       " 'get_in_dim',\n",
       " 'get_out_dim',\n",
       " 'get_pad',\n",
       " 'get_parallel',\n",
       " 'get_stride',\n",
       " 'get_type',\n",
       " 'ibits',\n",
       " 'obits',\n",
       " 'thresholds',\n",
       " 'updateBitwidths']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showMembers(streamlined_net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1. ...  1.  1.  1.]\n",
      " [-1.  1.  1. ...  1.  1.  1.]\n",
      " [ 1.  1. -1. ... -1.  1.  1.]\n",
      " ...\n",
      " [ 1.  1. -1. ...  1.  1.  1.]\n",
      " [ 1. -1.  1. ... -1.  1. -1.]\n",
      " [-1. -1. -1. ...  1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "# inspect weights of first FC layer\n",
    "print streamlined_net[1].W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
