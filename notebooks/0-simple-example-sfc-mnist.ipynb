{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From HWGQ-Caffe to HLS: A simple FINN example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**WARNING: FINN is undergoing a major overhaul at the moment, so the information below (especially the implementation details) is subject to change.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a pre-trained, binarized network that comes with FINN. As we'll see in a moment, it receives 784 inputs (which is the size of one image in the MNIST dataset), transforms it with three layers of 256 neurons, then produces 10 outputs (one for each digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 1344691 Jun 11 14:22 /app/FINN/../FINN/FINN/inputs/sfc-w1a1.caffemodel\n",
      "-rw-r--r-- 1 root root 2964 Feb 20 14:52 /app/FINN/../FINN/FINN/inputs/sfc-w1a1.prototxt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "FINN_ROOT = os.environ['FINN_ROOT']\n",
    "params = FINN_ROOT + \"/inputs/sfc-w1a1.caffemodel\"\n",
    "topology = FINN_ROOT + \"/inputs/sfc-w1a1.prototxt\"\n",
    "! ls -l $params\n",
    "! ls -l $topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"sfc-w1a1-hwgq\"\r\n",
      "input: \"data\"\r\n",
      "input_shape {\r\n",
      "  dim: 64\r\n",
      "  dim: 1\r\n",
      "  dim: 28\r\n",
      "  dim: 28\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"bn_inp\"\r\n",
      "  type: \"BatchNorm\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"bn_inp\"\r\n",
      "  param {\r\n",
      "    lr_mult: 0\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 0\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 0\r\n",
      "  }\r\n",
      "  batch_norm_param {\r\n",
      "    moving_average_fraction: 0.95\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"qt_inp\"\r\n",
      "  type: \"Quant\"\r\n",
      "  bottom: \"bn_inp\"\r\n",
      "  top: \"qt_inp\"\r\n",
      "  quant_param {\r\n",
      "    forward_func: \"sign\"\r\n",
      "    backward_func: \"hard_tanh\"\r\n",
      "    clip_thr: 1.0\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip1\"\r\n",
      "  type: \"BinaryInnerProduct\"\r\n",
      "  bottom: \"qt_inp\"\r\n",
      "  top: \"ip1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 256\r\n",
      "    bias_term: false\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.005\r\n",
      "    }\r\n",
      "  }\r\n",
      "  binary_inner_product_param {\r\n",
      "    use_alpha: true\r\n",
      "  }  \r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 58 $topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see how the input data is normalized and quantized, and the first binarized fully-connected (here called \"inner product\") layer. Tools such as [Netscope](https://ethereon.github.io/netscope/#/editor) can help understand the structure of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frontend: From Caffe to FINN IR\n",
    "The frontend stage is responsible for converting QNNs trained by a variety of frameworks to the FINN intermediate representation (IR). As each framework exposes their QNN topologies through custom formats, FINN must first perform a conversion to a common IR that it knows how to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import a trained network through the HWGQ-Caffe frontend\n",
    "import FINN.frontend.frontend_hwgq as fe\n",
    "imported_net = fe.importCaffeNetwork(topology, params)\n",
    "imported_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What functionality does the IR expose?\n",
    "Let's get acquainted with what the FINN IR looks like and try to look \"inside\" some of the layers. As FINN is open source you could simply look at the IR source code in `FINN/core/layers.py`, but here we'll use a little helper function to investigate the exposed members and functions dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMembers(layer):\n",
    "    return filter(lambda x: not x.startswith(\"_\"), dir(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current version of FINN IR, we only support linear topologies (i.e. no skip connections, just one layer after another). As such, there is no graph representation and parent/child pointers; the IR is simply a list of objects that correspond to supported neural network layers. We can look at different layers by accessing this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullyConnectedLayer"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imported_net[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W',\n",
       " 'execute',\n",
       " 'getInputSize',\n",
       " 'getNumOps',\n",
       " 'getOutputSize',\n",
       " 'getParamSize',\n",
       " 'getTotalInputBits',\n",
       " 'getTotalOutputBits',\n",
       " 'getTotalParamBits',\n",
       " 'get_filter_dim',\n",
       " 'get_in_dim',\n",
       " 'get_out_dim',\n",
       " 'get_pad',\n",
       " 'get_parallel',\n",
       " 'get_stride',\n",
       " 'get_type',\n",
       " 'ibits',\n",
       " 'in_dim',\n",
       " 'insize',\n",
       " 'kernel',\n",
       " 'obits',\n",
       " 'outsize',\n",
       " 'updateBitwidths',\n",
       " 'wbits']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showMembers(imported_net[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 784, outputs: 256\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs: %d, outputs: %d\" % (imported_net[2].getInputSize(), imported_net[2].getOutputSize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix shape: (256, 784) bits per weight: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight matrix shape: %s bits per weight: %d\" % (str(imported_net[2].W.shape), imported_net[2].wbits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, large parts of the FINN IR are *executable*, as you probably guessed from the `execute` function. At any point, we can generate a random vector of appropriate dimensions and pass it through this layer by calling `execute` to see what the output would be like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rand_inp_vec = np.random.randn(784)\n",
    "ret = imported_net[2].execute(rand_inp_vec)\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get an output vector of the expected size when we pass in an input vector of appropriate size for this layer.\n",
    "\n",
    "## The Streamlining Transform\n",
    "\n",
    "You may have noticed that our imported network contains a bunch of `LinearLayer` instances with floating point parameters (which is currently indicated in FINN IR as 32 bits), like this one:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer type: LinearLayer weight bits: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer type: %s weight bits: %d\" % (imported_net[3].get_type(), imported_net[3].wbits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So where did this come from in a binarized network? Many state-of-the-art BNN/QNN methods use some floating point computation in the forward pass to improve the accuracy. Some examples are batch normalization layers and channel-wise scaling factors. Although these layers do not typically contain a large amount of computation, they may still incur slowdowns on devices where floating point operations are expensive and increase the memory footprint of the QNN by adding floating point parameters. This is the case for creating \"dataflow-style\" FPGA accelerators, so we'd like to somehow get rid of those floating point parameters and operations. \n",
    "\n",
    "Fortunately, we can use what is referred to as [streamlining](https://arxiv.org/pdf/1709.04060.pdf) to do this, without losing any accuracy! Streamlining is implemented as a *transformation* in FINN: a function that takes in the FINN IR representation of a network, and returns a transformed FINN IR representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BipolarThresholdingLayer, FullyConnectedLayer, BipolarThresholdingLayer, FullyConnectedLayer, BipolarThresholdingLayer, FullyConnectedLayer, BipolarThresholdingLayer, FullyConnectedLayer, LinearLayer]\n",
      "Number of layers in original imported network: 17\n",
      "Number of layers in streamlined network: 9\n"
     ]
    }
   ],
   "source": [
    "# example of a device-neutral transform: streamlining\n",
    "import FINN.transforms.transformations as tf\n",
    "streamlined_net = tf.makeCromulent(imported_net)\n",
    "print(streamlined_net)\n",
    "print(\"Number of layers in original imported network: %d\" % len(imported_net))\n",
    "print(\"Number of layers in streamlined network: %d\" % len(streamlined_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all `LinearLayer`s besides the final one have disappeared. This is achieved by updating the thresholds of the network. You can read more about how this is done [here](https://arxiv.org/pdf/1709.04060.pdf), or by looking at the source code for this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input normalization coefficients: [0.01270615] [-0.42501277] \n",
      "Original input quantization threshold: [[0.]]\n",
      "Streamlined input quantization threshold: [[34]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original input normalization coefficients: %s %s \" % (str(imported_net[0].A), str(imported_net[0].B)))\n",
    "print(\"Original input quantization threshold: \" + str(imported_net[1].thresholds))\n",
    "print(\"Streamlined input quantization threshold: \" + str(streamlined_net[0].thresholds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Does it still work?\n",
    "\n",
    "How do we know that the transformed network still does the correct thing, i.e. classify the MNIST dataset as well as the original imported one did? In order to check this, we can make use of the execution capabilities of FINN IR. We start by importing some functions from the `FINN.core.coverification` module that lets us load the MNIST dataset, and display the first test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FINN.core.coverification as cov\n",
    "(X,Y)= cov.loadMNIST()\n",
    "sample_mnist_input=X[0][0]\n",
    "sample_mnist_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F9E15741E10>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL.Image\n",
    "PIL.Image.fromarray(sample_mnist_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the convenience function `FINN.core.nn.execPipeline` to check what the original imported and transformed IRs make out of this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FINN.core.nn as nn\n",
    "orig_res = nn.execPipeline(sample_mnist_input.flatten(), imported_net)[0]\n",
    "streamlined_res = nn.execPipeline(sample_mnist_input.flatten(), streamlined_net)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.22489643e-03, -9.47696529e-01, -5.63656226e-01, -4.29998435e-01,\n",
       "       -2.10290406e-01, -1.75468760e+00, -3.00302663e+00,  8.65674712e+00,\n",
       "       -3.76429997e+00,  1.67605694e+00])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.22489643e-03, -9.47696529e-01, -5.63656226e-01, -4.29998435e-01,\n",
       "       -2.10290406e-01, -1.75468760e+00, -3.00302663e+00,  8.65674712e+00,\n",
       "       -3.76429997e+00,  1.67605694e+00])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamlined_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(orig_res, streamlined_res).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original prediction: 7 streamlined prediction: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Original prediction: %d streamlined prediction: %d\" % (np.argmax(orig_res), np.argmax(streamlined_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that streamlining *may* produce slightly different results for large/deep networks due to how floating point results are influenced by rounding and order of operations, this is why we use np.isclose instead of comparing with the equals operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From FINN IR to HLS\n",
    "\n",
    "FINN comes with a Vivado HLS library of components that lets us deploy QNNs as dataflow accelerators on FPGAs. We can use the FINN FPGA dataflow backend to generate an HLS implementation that makes use of this library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FPGABipolarMatrixThresholdLayer,\n",
       " FPGABipolarMatrixThresholdLayer,\n",
       " FPGABipolarMatrixThresholdLayer,\n",
       " FPGABipolarMatrixLayer]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FINN.backend.fpga.backend_fpga as be\n",
    "part_for_synthesis = streamlined_net[1:-1]\n",
    "fpga_dataflow_net = be.prepare(part_for_synthesis, ibits=1)\n",
    "fpga_dataflow_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FPGABipolarMatrixThresholdLayer,\n",
       " FPGABufferLayer,\n",
       " FPGABipolarMatrixThresholdLayer,\n",
       " FPGABufferLayer,\n",
       " FPGABipolarMatrixThresholdLayer,\n",
       " FPGABufferLayer,\n",
       " FPGABipolarMatrixLayer]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpga_dataflow_net = be.insert_buffers(fpga_dataflow_net, 1)\n",
    "fpga_dataflow_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bram_cost',\n",
       " 'canIncreaseMMV',\n",
       " 'canIncreasePE',\n",
       " 'canIncreaseSIMD',\n",
       " 'codegen_architecture',\n",
       " 'codegen_declarations',\n",
       " 'codegen_globals',\n",
       " 'codegen_outstream2single',\n",
       " 'codegen_params',\n",
       " 'codegen_single2instream',\n",
       " 'execute',\n",
       " 'getAccWidth',\n",
       " 'getIBits',\n",
       " 'getIBufName',\n",
       " 'getIGroup',\n",
       " 'getInStreamDecl',\n",
       " 'getInStreamW',\n",
       " 'getInputElemDType',\n",
       " 'getInputSize',\n",
       " 'getInputStreamDType',\n",
       " 'getMMV',\n",
       " 'getMatrixDims',\n",
       " 'getMaxMMV',\n",
       " 'getMaxPE',\n",
       " 'getMaxSIMD',\n",
       " 'getNumInputElems',\n",
       " 'getNumOps',\n",
       " 'getNumOutputElems',\n",
       " 'getOBits',\n",
       " 'getOBufName',\n",
       " 'getOGroup',\n",
       " 'getOutStreamDecl',\n",
       " 'getOutStreamW',\n",
       " 'getOutputElemDType',\n",
       " 'getOutputSize',\n",
       " 'getOutputStreamDType',\n",
       " 'getPE',\n",
       " 'getParamFileName',\n",
       " 'getParamSuffix',\n",
       " 'getSIMD',\n",
       " 'getT',\n",
       " 'getTMemCount',\n",
       " 'getTMemDType',\n",
       " 'getTMemName',\n",
       " 'getW',\n",
       " 'getWBits',\n",
       " 'getWMemCount',\n",
       " 'getWMemDType',\n",
       " 'getWMemName',\n",
       " 'get_filter_dim',\n",
       " 'get_in_dim',\n",
       " 'get_out_dim',\n",
       " 'get_pad',\n",
       " 'get_parallel',\n",
       " 'get_stride',\n",
       " 'get_type',\n",
       " 'ibits',\n",
       " 'inBufName',\n",
       " 'insize',\n",
       " 'instreamW',\n",
       " 'isBipolarTimesBipolar',\n",
       " 'isBipolarTimesRegular',\n",
       " 'isOSigned',\n",
       " 'kernel',\n",
       " 'layer_cycles',\n",
       " 'layer_ops',\n",
       " 'lut_cost',\n",
       " 'mlayer',\n",
       " 'mmv',\n",
       " 'name',\n",
       " 'obits',\n",
       " 'ops_per_cycle',\n",
       " 'outBufName',\n",
       " 'outsize',\n",
       " 'pe',\n",
       " 'simd',\n",
       " 'stride',\n",
       " 'tlayer',\n",
       " 'updateBitwidths',\n",
       " 'wbits']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showMembers(fpga_dataflow_net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FCLayer_Batch<64, 1, 1, 1, 16, 784, 256, 200704, 256>(inStream, FPGABufferLayer_1, weights_FPGABipolarMatrixThresholdLayer_0, thres_FPGABipolarMatrixThresholdLayer_0, numReps);\n"
     ]
    }
   ],
   "source": [
    "print(fpga_dataflow_net[0].codegen_architecture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        #pragma HLS ARRAY_PARTITION variable=weights_FPGABipolarMatrixThresholdLayer_0 complete dim=1\n",
      "        #pragma HLS ARRAY_PARTITION variable=thres_FPGABipolarMatrixThresholdLayer_0 complete dim=1\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(fpga_dataflow_net[0].codegen_declarations())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
