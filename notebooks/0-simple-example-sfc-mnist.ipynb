{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From HWGQ-Caffe to HLS: A simple FINN example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**WARNING: FINN is undergoing a major overhaul at the moment, so the information below (especially the implementation details) is subject to change.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a pre-trained, binarized network that comes with FINN. As we'll see in a moment, it receives 784 inputs (which is the size of one image in the MNIST dataset), transforms it with three layers of 256 neurons, then produces 10 outputs (one for each digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 1344691 Jun 11 14:22 /app/FINN/../FINN/FINN/inputs/sfc-w1a1.caffemodel\n",
      "-rw-r--r-- 1 root root 2964 Feb 20 14:52 /app/FINN/../FINN/FINN/inputs/sfc-w1a1.prototxt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "FINN_ROOT = os.environ['FINN_ROOT']\n",
    "params = FINN_ROOT + \"/inputs/sfc-w1a1.caffemodel\"\n",
    "topology = FINN_ROOT + \"/inputs/sfc-w1a1.prototxt\"\n",
    "! ls -l $params\n",
    "! ls -l $topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"sfc-w1a1-hwgq\"\r\n",
      "input: \"data\"\r\n",
      "input_shape {\r\n",
      "  dim: 64\r\n",
      "  dim: 1\r\n",
      "  dim: 28\r\n",
      "  dim: 28\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"bn_inp\"\r\n",
      "  type: \"BatchNorm\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"bn_inp\"\r\n",
      "  param {\r\n",
      "    lr_mult: 0\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 0\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 0\r\n",
      "  }\r\n",
      "  batch_norm_param {\r\n",
      "    moving_average_fraction: 0.95\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"qt_inp\"\r\n",
      "  type: \"Quant\"\r\n",
      "  bottom: \"bn_inp\"\r\n",
      "  top: \"qt_inp\"\r\n",
      "  quant_param {\r\n",
      "    forward_func: \"sign\"\r\n",
      "    backward_func: \"hard_tanh\"\r\n",
      "    clip_thr: 1.0\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip1\"\r\n",
      "  type: \"BinaryInnerProduct\"\r\n",
      "  bottom: \"qt_inp\"\r\n",
      "  top: \"ip1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 256\r\n",
      "    bias_term: false\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.005\r\n",
      "    }\r\n",
      "  }\r\n",
      "  binary_inner_product_param {\r\n",
      "    use_alpha: true\r\n",
      "  }  \r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 58 $topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see how the input data is normalized and quantized, and the first binarized fully-connected (here called \"inner product\") layer. Tools such as [Netscope](https://ethereon.github.io/netscope/#/editor) can help understand the structure of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frontend: From Caffe to FINN IR\n",
    "The frontend stage is responsible for converting QNNs trained by a variety of frameworks to the FINN intermediate representation (IR). As each framework exposes their QNN topologies through custom formats, FINN must first perform a conversion to a common IR that it knows how to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer,\n",
       " BipolarThresholdingLayer,\n",
       " FullyConnectedLayer,\n",
       " LinearLayer,\n",
       " LinearLayer]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import a trained network through the HWGQ-Caffe frontend\n",
    "import FINN.frontend.frontend_hwgq as fe\n",
    "imported_net = fe.importCaffeNetwork(topology, params)\n",
    "imported_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What functionality does the IR expose?\n",
    "Let's get acquainted with what the FINN IR looks like and try to look \"inside\" some of the layers. As FINN is open source you could simply look at the IR source code in `FINN/core/layers.py`, but here we'll use a little helper function to investigate the exposed members and functions dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMembers(layer):\n",
    "    return filter(lambda x: not x.startswith(\"_\"), dir(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current version of FINN IR, we only support linear topologies (i.e. no skip connections, just one layer after another). As such, there is no graph representation and parent/child pointers; the IR is simply a list of objects that correspond to supported neural network layers. We can look at different layers by accessing this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullyConnectedLayer"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imported_net[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W',\n",
       " 'execute',\n",
       " 'getInputSize',\n",
       " 'getNumOps',\n",
       " 'getOutputSize',\n",
       " 'getParamSize',\n",
       " 'getTotalInputBits',\n",
       " 'getTotalOutputBits',\n",
       " 'getTotalParamBits',\n",
       " 'get_filter_dim',\n",
       " 'get_in_dim',\n",
       " 'get_out_dim',\n",
       " 'get_pad',\n",
       " 'get_parallel',\n",
       " 'get_stride',\n",
       " 'get_type',\n",
       " 'ibits',\n",
       " 'in_dim',\n",
       " 'insize',\n",
       " 'kernel',\n",
       " 'obits',\n",
       " 'outsize',\n",
       " 'updateBitwidths',\n",
       " 'wbits']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showMembers(imported_net[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 784, outputs: 256\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs: %d, outputs: %d\" % (imported_net[2].getInputSize(), imported_net[2].getOutputSize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix shape: (256, 784) bits per weight: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight matrix shape: %s bits per weight: %d\" % (str(imported_net[2].W.shape), imported_net[2].wbits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, large parts of the FINN IR are *executable*, as you probably guessed from the `execute` function. At any point, we can generate a random vector of appropriate dimensions and pass it through this layer by calling `execute` to see what the output would be like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rand_inp_vec = np.random.randn(784)\n",
    "ret = imported_net[2].execute(rand_inp_vec)\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get an output vector of the expected size when we pass in an input vector of appropriate size for this layer. Let's have a quick look at what other layers are defined in the FINN IR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BipolarThresholdingLayer',\n",
       " 'ChanDeinterleaveLayer',\n",
       " 'ChanInterleaveLayer',\n",
       " 'ConvolutionLayer',\n",
       " 'DummyLayer',\n",
       " 'ExternalExecutionLayer',\n",
       " 'FullyConnectedLayer',\n",
       " 'Layer',\n",
       " 'LinearLayer',\n",
       " 'MatrixThresholdLayer',\n",
       " 'MonitorLayer',\n",
       " 'PaddingLayer',\n",
       " 'PoolingLayer',\n",
       " 'ReLULayer',\n",
       " 'SlidingWindowLayer',\n",
       " 'SoftmaxLayer',\n",
       " 'ThresholdingLayer',\n",
       " 'isConvLayer',\n",
       " 'isFCLayer',\n",
       " 'isLinearLayer',\n",
       " 'isMatrixLayer',\n",
       " 'isMatrixThresholdLayer',\n",
       " 'isMaxPoolingLayer',\n",
       " 'isPoolingLayer',\n",
       " 'isReLULayer',\n",
       " 'isScalarLinearLayer',\n",
       " 'isSoftmaxLayer',\n",
       " 'isThresholdLayer']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FINN.core.layers as lb\n",
    "filter(lambda x: x.endswith(\"Layer\"), dir(lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Streamlining Transform\n",
    "\n",
    "You may have noticed that our imported network contains a bunch of `LinearLayer` instances with floating point parameters (which is currently indicated in FINN IR as 32 bits), like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer type: LinearLayer weight bits: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer type: %s weight bits: %d\" % (imported_net[3].get_type(), imported_net[3].wbits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So where did this come from in a binarized network? Many state-of-the-art BNN/QNN methods use some floating point computation in the forward pass to improve the accuracy. Some examples are batch normalization layers and channel-wise scaling factors. Although these layers do not typically contain a large amount of computation, they may still incur slowdowns on devices where floating point operations are expensive and increase the memory footprint of the QNN by adding floating point parameters. This is the case for creating \"dataflow-style\" FPGA accelerators, so we'd like to somehow get rid of those floating point parameters and operations. \n",
    "\n",
    "Fortunately, we can use what is referred to as [streamlining](https://arxiv.org/pdf/1709.04060.pdf) to do this, without losing any accuracy! Streamlining is implemented as a *transformation* in FINN: a function that takes in the FINN IR representation of a network, and returns a transformed FINN IR representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BipolarThresholdingLayer, FullyConnectedLayer, BipolarThresholdingLayer, FullyConnectedLayer, BipolarThresholdingLayer, FullyConnectedLayer, BipolarThresholdingLayer, FullyConnectedLayer, LinearLayer]\n",
      "Number of layers in original imported network: 17\n",
      "Number of layers in streamlined network: 9\n"
     ]
    }
   ],
   "source": [
    "# example of a device-neutral transform: streamlining\n",
    "import FINN.transforms.transformations as tf\n",
    "streamlined_net = tf.makeCromulent(imported_net)\n",
    "print(streamlined_net)\n",
    "print(\"Number of layers in original imported network: %d\" % len(imported_net))\n",
    "print(\"Number of layers in streamlined network: %d\" % len(streamlined_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all `LinearLayer`s besides the final one have disappeared. This is achieved by updating the thresholds of the network. You can read more about how this is done [here](https://arxiv.org/pdf/1709.04060.pdf), or by looking at the source code for this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input normalization coefficients: [0.01270615] [-0.42501277] \n",
      "Original input quantization threshold: [[0.]]\n",
      "Streamlined input quantization threshold: [[34]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original input normalization coefficients: %s %s \" % (str(imported_net[0].A), str(imported_net[0].B)))\n",
    "print(\"Original input quantization threshold: \" + str(imported_net[1].thresholds))\n",
    "print(\"Streamlined input quantization threshold: \" + str(streamlined_net[0].thresholds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Does it still work?\n",
    "\n",
    "How do we know that the transformed network still does the correct thing, i.e. classify the MNIST dataset as well as the original imported one did? In order to check this, we can make use of the execution capabilities of FINN IR. We start by importing some functions from the `FINN.core.coverification` module that lets us load the MNIST dataset, and display the first test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FINN.core.coverification as cov\n",
    "(X,Y)= cov.loadMNIST()\n",
    "sample_mnist_input=X[0][0]\n",
    "sample_mnist_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F511DCB6F10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL.Image\n",
    "PIL.Image.fromarray(sample_mnist_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the convenience function `FINN.core.nn.execPipeline` to check what the original imported and transformed IRs make out of this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FINN.core.nn as nn\n",
    "orig_res = nn.execPipeline(sample_mnist_input.flatten(), imported_net)[0]\n",
    "streamlined_res = nn.execPipeline(sample_mnist_input.flatten(), streamlined_net)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.22489643e-03, -9.47696529e-01, -5.63656226e-01, -4.29998435e-01,\n",
       "       -2.10290406e-01, -1.75468760e+00, -3.00302663e+00,  8.65674712e+00,\n",
       "       -3.76429997e+00,  1.67605694e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.22489643e-03, -9.47696529e-01, -5.63656226e-01, -4.29998435e-01,\n",
       "       -2.10290406e-01, -1.75468760e+00, -3.00302663e+00,  8.65674712e+00,\n",
       "       -3.76429997e+00,  1.67605694e+00])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamlined_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(orig_res, streamlined_res).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original prediction: 7 streamlined prediction: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Original prediction: %d streamlined prediction: %d\" % (np.argmax(orig_res), np.argmax(streamlined_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that streamlining *may* produce slightly different results for large/deep networks due to how floating point results are influenced by rounding and order of operations, this is why we use np.isclose instead of comparing with the equals operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From FINN IR to HLS: It's Transforms All the Way Down\n",
    "\n",
    "FINN comes with a Vivado HLS library of components that lets us deploy QNNs as dataflow accelerators on FPGAs. We can use the FINN FPGA dataflow backend to generate an HLS implementation that makes use of this library. The FPGA dataflow backend actually has a convenience call `synthesize()` that handles all the intermediate steps of going from FINN IR to HLS, but we explicitly make step-by-step calls here to show the details of how the backend works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def showSrc(what):\n",
    "    print(\"\".join(inspect.getsourcelines(what)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have a look at the `prepare` function that the FPGA dataflow backend exposes to see what kind of steps are taken to go from the device-neutral FINN IR all the way down to HLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def prepare(pipeline, ibits):\n",
      "    \"Convert given QNN into a form processable by the FPGA backend.\"\n",
      "    # interleave channels\n",
      "    pipeline = trns.apply_repeated(pipeline, trns.passInterleaveChannels)\n",
      "    # fuse activations to easily detect matrix-threshold op pairs\n",
      "    pipeline = trns.apply_repeated(pipeline, trns.passFuseActivations)\n",
      "    # compute/update bitwidths, using the specified # of input bits\n",
      "    myUpdateBitwidths = lambda x: trns.passUpdateBitwidths(x, ibits)\n",
      "    pipeline = trns.apply_repeated(pipeline, myUpdateBitwidths)\n",
      "    # convert all layers to their Caffe implementation variants\n",
      "    pipeline = trns.apply_repeated(pipeline, passConvertToFPGALayers)\n",
      "    # give names to each layer -- useful for e.g. interactive PE/SIMD setting\n",
      "    pipeline = trns.apply_repeated(pipeline, trns.passGiveUniqueNames)\n",
      "    # sanity check\n",
      "    pipeline = trns.apply_repeated(pipeline, passCheckProduceConsume)\n",
      "    return pipeline\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showSrc(be.prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that many transforms are applied in the process (and this is not all of them, you can see more in the `synthesize` function call). Every step brings the original IR description a little closer to the function calls available in the FINN dataflow HLS library. For instance, have a look at the `passFuseActivations` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def passFuseActivations(pipeline):\n",
      "    \"Replace (Matrix, Threshold) layer pairs with fused equivalents.\"\n",
      "    inStages = pipeline\n",
      "    inStages.reverse()\n",
      "    numChanges = 0\n",
      "    ret = []\n",
      "    while len(inStages) > 1:\n",
      "        layerA = inStages.pop()\n",
      "        layerB = inStages.pop()\n",
      "        if lb.isMatrixLayer(layerA) and lb.isThresholdLayer(layerB):\n",
      "            ret += [lb.MatrixThresholdLayer(\"\", layerA, layerB)]\n",
      "            numChanges += 1\n",
      "        else:\n",
      "            ret += [layerA]\n",
      "            inStages.append(layerB)\n",
      "    # pop final element, if any left\n",
      "    if len(inStages) == 1:\n",
      "        ret += [inStages.pop()]\n",
      "    return (ret, numChanges)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showSrc(tf.passFuseActivations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular pass, as expressed in the docstring, simply replaces pairs of linear transform and thresholding layers with a single layer (`MatrixThresholdLayer`) that \"contains\" both. How does this help? The FINN dataflow HLS library exposes a primitive (the so-called MVTU, or Matrix-Vector-Threshold Unit) that implements these operations. By gradually translating the higher-level representation into something that resembles the HLS library call, we get closer to our goal. Let's look at the source for transform, the `passConvertToFPGALayers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def passConvertToFPGALayers(pipeline):\n",
      "    \"Convert supported layers to their FPGA dataflow layer equivalents.\"\n",
      "    ret = []\n",
      "    numChanges = 0\n",
      "    for L in pipeline:\n",
      "        if L.get_type().startswith(\"FPGA\"):\n",
      "            # already FPGA layer -- copy as-is\n",
      "            ret += [L]\n",
      "        elif layers_base.isMatrixThresholdLayer(L):\n",
      "            if layers_base.isFCLayer(L.mlayer):\n",
      "                # TODO the conditions need to be more specific (bipolar\n",
      "                # weights)\n",
      "                ret += [layers_fpga.FPGABipolarMatrixThresholdLayer(L)]\n",
      "            elif layers_base.isConvLayer(L.mlayer):\n",
      "                # TODO the conditions need to be more specific (bipolar\n",
      "                # weights)\n",
      "                ret += [layers_fpga.FPGABipolarConvThresholdLayer(L)]\n",
      "            else:\n",
      "                raise Exception(\"Unsupported matrix-threshold combination for FPGA backend\")\n",
      "            numChanges += 1\n",
      "        elif layers_base.isFCLayer(L):\n",
      "            # TODO the conditions need to be more specific (bipolar input and\n",
      "            # weights)\n",
      "            ret += [layers_fpga.FPGABipolarMatrixLayer(L)]\n",
      "            numChanges += 1\n",
      "        elif layers_base.isPoolingLayer(L):\n",
      "            ret += [layers_fpga.FPGAMaxPoolLayer(L)]\n",
      "        else:\n",
      "            raise Exception(\"Unsupported layer type in FPGA backend: %s\" % L.get_type())\n",
      "    return (ret, numChanges)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showSrc(be.passConvertToFPGALayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pass is actually quite straightforward; it replaces instances of device-neutral FINN IR layers (such as `MatrixThresholdLayer`) with equivalent FPGA-backend-specific layers (such as `FPGAMatrixThresholdLayer`). If it encounters an unsupported layer it simply raises an exception. Let's now call the `prepare()` function and examine its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FPGABipolarMatrixThresholdLayer,\n",
       " FPGABipolarMatrixThresholdLayer,\n",
       " FPGABipolarMatrixThresholdLayer,\n",
       " FPGABipolarMatrixLayer]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FINN.backend.fpga.backend_fpga as be\n",
    "part_for_synthesis = streamlined_net[1:-1]\n",
    "fpga_dataflow_net = be.prepare(part_for_synthesis, ibits=1)\n",
    "fpga_dataflow_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bram_cost',\n",
       " 'canIncreaseMMV',\n",
       " 'canIncreasePE',\n",
       " 'canIncreaseSIMD',\n",
       " 'codegen_architecture',\n",
       " 'codegen_declarations',\n",
       " 'codegen_globals',\n",
       " 'codegen_outstream2single',\n",
       " 'codegen_params',\n",
       " 'codegen_single2instream',\n",
       " 'execute',\n",
       " 'getAccWidth',\n",
       " 'getIBits',\n",
       " 'getIBufName',\n",
       " 'getIGroup',\n",
       " 'getInStreamDecl',\n",
       " 'getInStreamW',\n",
       " 'getInputElemDType',\n",
       " 'getInputSize',\n",
       " 'getInputStreamDType',\n",
       " 'getMMV',\n",
       " 'getMatrixDims',\n",
       " 'getMaxMMV',\n",
       " 'getMaxPE',\n",
       " 'getMaxSIMD',\n",
       " 'getNumInputElems',\n",
       " 'getNumOps',\n",
       " 'getNumOutputElems',\n",
       " 'getOBits',\n",
       " 'getOBufName',\n",
       " 'getOGroup',\n",
       " 'getOutStreamDecl',\n",
       " 'getOutStreamW',\n",
       " 'getOutputElemDType',\n",
       " 'getOutputSize',\n",
       " 'getOutputStreamDType',\n",
       " 'getPE',\n",
       " 'getParamFileName',\n",
       " 'getParamSuffix',\n",
       " 'getSIMD',\n",
       " 'getT',\n",
       " 'getTMemCount',\n",
       " 'getTMemDType',\n",
       " 'getTMemName',\n",
       " 'getW',\n",
       " 'getWBits',\n",
       " 'getWMemCount',\n",
       " 'getWMemDType',\n",
       " 'getWMemName',\n",
       " 'get_filter_dim',\n",
       " 'get_in_dim',\n",
       " 'get_out_dim',\n",
       " 'get_pad',\n",
       " 'get_parallel',\n",
       " 'get_stride',\n",
       " 'get_type',\n",
       " 'ibits',\n",
       " 'insize',\n",
       " 'instreamW',\n",
       " 'isBipolarTimesBipolar',\n",
       " 'isBipolarTimesRegular',\n",
       " 'isOSigned',\n",
       " 'kernel',\n",
       " 'layer_cycles',\n",
       " 'layer_ops',\n",
       " 'lut_cost',\n",
       " 'mlayer',\n",
       " 'mmv',\n",
       " 'name',\n",
       " 'obits',\n",
       " 'ops_per_cycle',\n",
       " 'outsize',\n",
       " 'pe',\n",
       " 'simd',\n",
       " 'stride',\n",
       " 'tlayer',\n",
       " 'updateBitwidths',\n",
       " 'wbits']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showMembers(fpga_dataflow_net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the functionality exposed by the `FPGABipolarMatrixThresholdLayer` is significantly different from that of the `MatrixThresholdLayer`: namely, the backend-specific layer type contains many functions and members that control the details of the implementation, such as the folding factors (how much time multiplexing is done across each axis) controlled by the `pe, simd, mmv` parameters. We also see that this layer exposes certain code generation functions like these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FCLayer_Batch<64, 1, 1, 1, 16, 784, 256, 200704, 256>(inStream, FPGABufferLayer_1, weights_FPGABipolarMatrixThresholdLayer_0, thres_FPGABipolarMatrixThresholdLayer_0, numReps);\n"
     ]
    }
   ],
   "source": [
    "print(fpga_dataflow_net[0].codegen_architecture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        #pragma HLS ARRAY_PARTITION variable=weights_FPGABipolarMatrixThresholdLayer_0 complete dim=1\n",
      "        #pragma HLS ARRAY_PARTITION variable=thres_FPGABipolarMatrixThresholdLayer_0 complete dim=1\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(fpga_dataflow_net[0].codegen_declarations())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
